{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "import evaluate\n",
    "\n",
    "wer  = evaluate.load('wer')\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "def down_sample_audio(audio_original, original_sample_rate):\n",
    "    target_sample_rate = 16000\n",
    "\n",
    "    # Calculate the number of samples for the target sample rate\n",
    "    num_samples = int(len(audio_original) * target_sample_rate / original_sample_rate)\n",
    "\n",
    "    # Resample the audio array to the target sample rate\n",
    "    downsampled_audio = resample(audio_original, num_samples)\n",
    "\n",
    "    return downsampled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atco_asr_data = load_dataset('parquet',data_files=\"train-00000-of-00005-c6681348ac8543dc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",language='english',task='transcribe')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\",language='english',task='transcribe')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class whisper_training_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len):#daatset is huggingface dataset object\n",
    "        self.dataset = dataset\n",
    "        self.max_len = max_len\n",
    "        self.bos_token = model.config.decoder_start_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "\n",
    "        input_features = feature_extractor(item['audio'][\"array\"], sampling_rate=16000,return_tensors='pt').input_features[0]\n",
    "\n",
    "        # Process the transcription\n",
    "        transcription = item[\"text\"]\n",
    "\n",
    "        # Create labels\n",
    "        labels = tokenizer(transcription, padding=\"max_length\", max_length=self.max_len, truncation=True, return_tensors=\"pt\")\n",
    "        labels = labels[\"input_ids\"].masked_fill(labels['attention_mask'].ne(1), -100)\n",
    "        labels = labels[0][1:]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = whisper_training_dataset(dataset=atco_asr_data['train'], max_len=60)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust batch size as needed\n",
    "    shuffle=True,  # Shuffle data during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "max_steps=100\n",
    "step=0\n",
    "\n",
    "running_loss=[]\n",
    "\n",
    "progress_bar = tqdm(range(max_steps),leave=False)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        input_features, labels = batch[\"input_features\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_features, labels=labels)  # Assuming your model takes these inputs\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        step=step+1\n",
    "        \n",
    "        if step%10==0:\n",
    "        \n",
    "            plt.plot(running_loss)\n",
    "            clear_output(wait=True)\n",
    "            plt.show()\n",
    "            \n",
    "        if step==max_steps:\n",
    "\n",
    "            break\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'finetuned_atco.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "atco_asr_data = load_dataset('parquet',data_files=\"validation-00000-of-00002-7a5ea3756991bf72.parquet\")\n",
    "\n",
    "predictions=[]\n",
    "references=[]\n",
    "\n",
    "for sample in tqdm(atco_asr_data['train'],total=len(atco_asr_data['train'])):\n",
    "    audio=sample['audio']['array']\n",
    "    sample_rate=sample['audio']['sampling_rate']\n",
    "    text=sample['text']\n",
    "\n",
    "    audio = down_sample_audio(audio, sample_rate) # downsample the audio to 16000Hz for WHISPER\n",
    "\n",
    "    input_features = feature_extractor(\n",
    "    raw_speech=audio,\n",
    "    sampling_rate=16000,\n",
    "    return_tensors='pt',\n",
    "    padding=True).input_features\n",
    "\n",
    "    # Generate predictions with no gradient computation\n",
    "    with torch.no_grad():\n",
    "        op = model.generate(input_features.to('cuda'), language='english', task='transcribe')\n",
    "\n",
    "    # Decode predictions\n",
    "    text_preds = tokenizer.batch_decode(op, skip_special_tokens=True)\n",
    "\n",
    "    # Append batch predictions and references to the respective lists\n",
    "    predictions.extend(text_preds)\n",
    "    references.extend([text])\n",
    "\n",
    "print(f'The WER after training is {wer.compute(predictions=predictions, references=references) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
