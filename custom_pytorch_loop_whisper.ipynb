{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets>=2.6.1\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install librosa\n",
    "# !pip install evaluate>=0.30\n",
    "# !pip install jiwer\n",
    "# !pip install gradio\n",
    "# !pip install -q bitsandbytes datasets accelerate\n",
    "# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d17626dd1b14c43b9bfc3c75c39fc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name_or_path = \"openai/whisper-small\"\n",
    "language = \"Hindi\"\n",
    "language_abbr = \"or\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_17_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 2048\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 696\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", trust_remote_code=True)\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\",trust_remote_code=True)\n",
    "\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\",'variant']\n",
    ")\n",
    "\n",
    "print(common_voice)\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_sentence_length(dataset):\n",
    "  \"\"\"Finds the length of the longest sentence in the dataset.\n",
    "\n",
    "  Args:\n",
    "    dataset: The Hugging Face dataset.\n",
    "\n",
    "  Returns:\n",
    "    The length of the longest sentence.\n",
    "  \"\"\"\n",
    "\n",
    "  longest_sentence_length = 0\n",
    "  for item in dataset:\n",
    "    sentence_length = len(item[\"sentence\"])\n",
    "    if sentence_length > longest_sentence_length:\n",
    "      longest_sentence_length = sentence_length\n",
    "      long_sentence = item[\"sentence\"]\n",
    "  return longest_sentence_length,long_sentence\n",
    "\n",
    "longest_sentence_length,long_sentence = find_longest_sentence_length(common_voice[\"train\"])\n",
    "max_len = len(tokenizer(long_sentence)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor\n",
    "import librosa\n",
    "\n",
    "class CommonVoiceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, processor, model, dataset, max_len):#daatset is huggingface dataset object\n",
    "        self.processor = processor\n",
    "        self.dataset = dataset\n",
    "        self.max_len = max_len\n",
    "        self.bos_token = model.config.decoder_start_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "\n",
    "        input_features = processor.feature_extractor(item['audio'][\"array\"], sampling_rate=16000,return_tensors='pt').input_features[0]\n",
    "\n",
    "        # Process the transcription\n",
    "        transcription = item[\"sentence\"]\n",
    "\n",
    "        # Create labels\n",
    "        labels = self.processor.tokenizer(transcription, padding=\"max_length\", max_length=self.max_len, truncation=True, return_tensors=\"pt\")\n",
    "        labels = labels[\"input_ids\"].masked_fill(labels['attention_mask'].ne(1), -100)\n",
    "        labels = labels[0][1:]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=CommonVoiceDataset(processor=processor, model=model, dataset=common_voice[\"train\"], max_len=max_len)\n",
    "test_dataset=CommonVoiceDataset(processor=processor, model=model, dataset=common_voice[\"test\"], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,  # Adjust batch size as needed\n",
    "    shuffle=True,  # Shuffle data during training\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,  # Adjust batch size as needed\n",
    "    shuffle=False,  # No need to shuffle during testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration,  BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"Mohan-diffuser/whisper-large-v2-odia-100steps\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id,low_cpu_mem_usage=False,is_trainable=True)\n",
    "\n",
    "'''Override generation arguments - no tokens are forced as decoder outputs (see forced_decoder_ids),\n",
    "no tokens are suppressed during generation (see suppress_tokens):'''\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    # param=param.type(torch.float16)\n",
    "    if param.requires_grad==True:\n",
    "        print(name,param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/44 [01:15<26:16, 37.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     20\u001b[0m         generated_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 21\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     28\u001b[0m         )\n\u001b[0;32m     29\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     30\u001b[0m         labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\peft\\peft_model.py:840\u001b[0m, in \u001b[0;36mPeftModel.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    839\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m--> 840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_base_model()\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:684\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[1;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    677\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[0;32m    678\u001b[0m (\n\u001b[0;32m    679\u001b[0m     seek_sequences,\n\u001b[0;32m    680\u001b[0m     seek_outputs,\n\u001b[0;32m    681\u001b[0m     should_skip,\n\u001b[0;32m    682\u001b[0m     do_condition_on_prev_tokens,\n\u001b[0;32m    683\u001b[0m     model_output_type,\n\u001b[1;32m--> 684\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:847\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[1;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    843\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    844\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    845\u001b[0m         )\n\u001b[1;32m--> 847\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    848\u001b[0m     segment_input,\n\u001b[0;32m    849\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    850\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m    851\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m    852\u001b[0m     prefix_allowed_tokens_fn\u001b[38;5;241m=\u001b[39mprefix_allowed_tokens_fn,\n\u001b[0;32m    853\u001b[0m     synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m    854\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m    855\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    859\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\generation\\utils.py:2220\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2212\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2213\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2214\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2215\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2216\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2217\u001b[0m     )\n\u001b[0;32m   2219\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2221\u001b[0m         input_ids,\n\u001b[0;32m   2222\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2223\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2224\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2225\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2226\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2227\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2228\u001b[0m     )\n\u001b[0;32m   2230\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2233\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2234\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2240\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\generation\\utils.py:3228\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3225\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[1;32m-> 3228\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3230\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[0;32m   3231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\generation\\logits_process.py:104\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\generation\\logits_process.py:1869\u001b[0m, in \u001b[0;36mSuppressTokensLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m   1868\u001b[0m     vocab_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1869\u001b[0m     suppress_token_mask \u001b[38;5;241m=\u001b[39m \u001b[43misin_mps_friendly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1870\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(suppress_token_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), scores)\n\u001b[0;32m   1871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\pytorch_utils.py:328\u001b[0m, in \u001b[0;36misin_mps_friendly\u001b[1;34m(elements, test_elements)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements\u001b[38;5;241m.\u001b[39mtile(test_elements\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(test_elements\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "normalized_predictions = []\n",
    "normalized_references = []\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
    "\n",
    "            # break\n",
    "\n",
    "\n",
    "        del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
    "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
    "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
    "\n",
    "print(f\"{wer=} and {normalized_wer=}\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ସାତ!',\n",
       "  'ମା ବୁଢ଼ିକାରଇ ଶୁକାଇ ନାଳା ।',\n",
       "  'ନାଉରି କି ଶୋଇଯାଇଛି?',\n",
       "  'ସମସ୍ତ ପ୍ରକାର ଦାଶ ତ ଉ ଦାସ ବେବ ସାୟ ନିଶିଧ ହେବ ।',\n",
       "  'ଦୁମ୍ଦୁମ୍ପୁରରେ କନ୍ୟାଟି ଘର ଯୋଗା ହେଲାଣି ।',\n",
       "  'ଶୁତୁ ଉଫୁନିକ ପିଲାଟା, ଅଲଗା ଇପୋପିଆ ହୋଇ ଖଣ ରହିପାରିବୁ?',\n",
       "  'ଆରେ ମୁ ଚନ୍ଦ୍ର ଉଦିଆରେ, ମୁଁ ଘର ଅନ୍ଧାର କରି ଆସି ଚୁଅ, ଆରେ ମୁ କଳାମାଣିକରେ ।',\n",
       "  'ସାଆନ୍ତେ ଆପେ କହନ୍ତି, ସେ କାହାରି ଗୋଟିଏ ପଇସା ଆଣନ୍ତି ନାହିଁ ।',\n",
       "  'ନ୍ୟ ।',\n",
       "  'ଶୁନ ।',\n",
       "  'ନ!',\n",
       "  'ଅବଶ୍ୟ ଅବଶ୍ୟ, ଏ କଥାଟା କଣ କହିବାକୁ ହେପ?',\n",
       "  'ଆଗକୁ ମୁଁ ଯାଇ ପାରିଲି ଲାହିଁ ।',\n",
       "  'ବାହା ହେଲା ହଁ, ଢେର ଦିନଯାଏ ସନ୍ତାନ ମୁଖ ଦେଖିଲା ନାହିଁ ।',\n",
       "  'ଇଡ଼େ ବଡ଼ ଜଳ କ୍ର୍ତି ଯେ କରିପାରେ ଶେକି, ଆମମାନଙ୍କ ପରି ମନ୍ଲେଷ ।',\n",
       "  'ପିତା ଶୁଣି ପବନ ଲାଗିଛି ।'],\n",
       " ['ସାତ',\n",
       "  'ମାଆ ବୁଢ଼ୀ କରଇ ସୁକଇଲାଣ',\n",
       "  'ନାଉରି କି ଶୋଇ ଯାଇଛି?',\n",
       "  'ସମସ୍ତ ପ୍ରକାର ଦାସତ୍ତ୍ୱ ଓ ଦାସବ୍ୟବସାୟ ନିଷିଦ୍ଧ ହେବ ।',\n",
       "  'ଦୁମଦୁମପୁରରେ କନ୍ୟାଟି ଘରଯୋଗା ହେଲାଣି ।',\n",
       "  'ତୁ ଉଛୁଣିକା ପିଲାଟା, ଅଲଗା ଏକୁଟିଆ ହୋଇ କଣ ରହି ପାରିବୁ?',\n",
       "  'ଆରେ ମୋ ଚନ୍ଦ୍ର ଉଦିଆରେ, ମୋଘର ଅନ୍ଧାର କରି ଆସିଛୁ, ଆରେ ମୋ କଳାମାଣିକରେ ।',\n",
       "  'ସାଆନ୍ତେ ଆପେ କହନ୍ତି ସେ କାହାରି ଗୋଟିଏ ପଇସା ଆଣନ୍ତି ନାହିଁ ।',\n",
       "  'ନା',\n",
       "  'ଶୂନ',\n",
       "  'ନଅ',\n",
       "  '\"ଅବଶ୍ୟ ଅବଶ୍ୟ, ଏ କଥାଟା କଣ କହିବାକୁ ହେବ?',\n",
       "  'ଆଗକୁ ମୁଁ ଯାଇପାରିଲି ନାହିଁ ।',\n",
       "  'ବାହା ହେଲା ହଁ, ଢେର ଦିନଯାଏ ସନ୍ତାନ ମୁଖ ଦେଖିଲା ନାହିଁ ।',\n",
       "  'ଏଡ଼େବଡ଼ ଜଳକୀର୍ତ୍ତି ଯେ କରିପାରେ, ସେ କି ଆମ୍ଭମାନଙ୍କପରି ମନୁଷ୍ୟ?',\n",
       "  'ପିତାସୁଣୀ ପବନ ଲାଗିଛି ।'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds,decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation for LoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n",
    "\n",
    "'''Override generation arguments - no tokens are forced as decoder outputs (see forced_decoder_ids),\n",
    "no tokens are suppressed during generation (see suppress_tokens):'''\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=16, lora_alpha=64, target_modules=[\"q_proj\", \"k_proj\",\"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\miniconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\miniconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\miniconda3\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\Asus\\miniconda3\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "model.config.use_cache=False # just for training, make it True during inference\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "max_steps=1000\n",
    "step=0\n",
    "eval_steps=100\n",
    "\n",
    "progress_bar = tqdm(range(max_steps),leave=False)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        input_features, labels = batch[\"input_features\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_features, labels=labels)  # Assuming your model takes these inputs\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        if (step+1) % eval_steps==0:\n",
    "\n",
    "          # Evaluate on the test set (optional)\n",
    "          model.eval()  # Set model to evaluation mode\n",
    "          peft_model_id = \"Mohan-diffuser/whisper-small-odia-3000steps\"\n",
    "          model.push_to_hub(peft_model_id)\n",
    "          test_loss = 0.0\n",
    "          with torch.no_grad():\n",
    "              for batch in test_dataloader:\n",
    "                  input_features, labels = batch[\"input_features\"].to(device), batch[\"labels\"].to(device)\n",
    "                  outputs = model(input_features, labels=labels)\n",
    "                  loss = outputs.loss\n",
    "                  test_loss += loss.item() * input_features.size(0)\n",
    "\n",
    "          test_loss /= len(test_dataloader.dataset)\n",
    "\n",
    "          # Print step results\n",
    "          print(f\"step {step + 1}/{max_steps}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "          model.train()  # Set model to training mode\n",
    "\n",
    "\n",
    "        step=step+1\n",
    "\n",
    "        if step==max_steps:\n",
    "\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
