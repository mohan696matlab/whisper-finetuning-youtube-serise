{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove only \", ', and ,\n",
    "    text = re.sub(r'[\",\\']', '', text)\n",
    "    # Optional: remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "from jiwer import cer\n",
    "\n",
    "\n",
    "\n",
    "def down_sample_audio(audio_original, original_sample_rate):\n",
    "    target_sample_rate = 16000\n",
    "\n",
    "    # Calculate the number of samples for the target sample rate\n",
    "    num_samples = int(len(audio_original) * target_sample_rate / original_sample_rate)\n",
    "\n",
    "    # Resample the audio array to the target sample rate\n",
    "    downsampled_audio = resample(audio_original, num_samples)\n",
    "\n",
    "    return downsampled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",language='bengali',task='transcribe')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\",language='bengali',task='transcribe')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('runs/lora_adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Mohan-diffuser/odia-english-ASR couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/mohan.dash/.cache/huggingface/datasets/Mohan-diffuser___odia-english-asr/default/0.0.0/f509c51d69de15af3428978aa79b490863f32d2f (last modified on Wed Apr 30 12:44:57 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,concatenate_datasets\n",
    "\n",
    "asr_dataset = load_dataset(\"Mohan-diffuser/odia-english-ASR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 62,\n",
       " 'num_samples': 159360,\n",
       " 'path': '/home/mohan.dash/.cache/huggingface/datasets/downloads/extracted/e2fcce8118681935ebfe4ee7e9b039155b764427d2269ffa5a66a0328fe9ec55/10020140876560357327.wav',\n",
       " 'audio': {'path': '10020140876560357327.wav',\n",
       "  'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00582969,\n",
       "         -0.00332552, -0.00426018]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'ତାଙ୍କର 2 ଘଣ୍ଟାର ଭାଷଣରେ ସେ କହିଥିଲେ ଯେ ଆଜି apple ତାର ଫୋନକୁ ପୁଣି ଉଦ୍ଭାବନ କରିବାକୁ ଯାଉଛି ଆମେ ଆଜି ଇତିହାସ ରଚିବାକୁ ଯାଉଛୁ',\n",
       " 'raw_transcription': 'ତାଙ୍କର 2 ଘଣ୍ଟାର ଭାଷଣରେ ସେ କହିଥିଲେ ଯେ \"ଆଜି Apple ତାର ଫୋନକୁ ପୁଣି ଉଦ୍ଭାବନ କରିବାକୁ ଯାଉଛି, ଆମେ ଆଜି ଇତିହାସ ରଚିବାକୁ ଯାଉଛୁ।\"',\n",
       " 'gender': 1,\n",
       " 'lang_id': 72,\n",
       " 'language': 'Oriya',\n",
       " 'lang_group_id': 4,\n",
       " 'eng_translation': 'In his 2-hour speech, he said that today Apple is going to reinvent its phone. We are going to make history today.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_transcription_lengths = []\n",
    "\n",
    "tokenized_text = tokenizer(asr_dataset['train']['transcription']).input_ids\n",
    "\n",
    "for text in tokenized_text:\n",
    "    list_of_transcription_lengths.append(len(text))\n",
    "    # break\n",
    "\n",
    "plt.hist(list_of_transcription_lengths)\n",
    "plt.xlabel(\"sentence length\")\n",
    "plt.ylabel(\"number of transcripts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------0------\n",
      "true : ତାଙ୍କର 2 ଘଣ୍ଟାର ଭାଷଣରେ ସେ କହିଥିଲେ ଯେ ଆଜି apple ତାର ଫୋନକୁ ପୁଣି ଉଦ୍ଭାବନ କରିବାକୁ ଯାଉଛି ଆମେ ଆଜି ଇତିହାସ ରଚିବାକୁ ଯାଉଛୁ \n",
      "pred :  तांकर तुए गड़ार बासुनरे से कवितिले जे आप्प्रिल तार पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्पुन्प�\n",
      "\n",
      " \n",
      "-------1------\n",
      "true : ଖେଳଗୁଡିକ ସକାଳ 10:00 ରେ ଭଲ ପାଗ ସହିତ ଆରମ୍ଭ ହୋଇଥିଲା ଏବଂ ମଧ୍ୟରାତ୍ରିର ଝଡ଼ ବର୍ଷା ବ୍ୟତୀତ ଯାହା ଶୀଘ୍ର ସଫା ହୋଇଯାଇଥିଲା ଏହା 7 ମ ରଗବୀ ପାଇଁ ଏକ ଉପଯୁକ୍ତ ଦିନ ଥିଲା \n",
      "pred :  ḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍḍ\n",
      "\n",
      " \n",
      "-------2------\n",
      "true : ଯଦି ଜଣେ ଆରୋହୀଙ୍କୁ ଘୋଡ଼ାରୁ ଫୋପାଡି ଦିଆଯାଏ କିନ୍ତୁ ଗୋଡ଼ କୁଣ୍ଡାରେ ଅଟକିଯାଏ ତେବେ ଘୋଡ଼ା ଦୌଡ଼ି ପଳାଇଲେ ସେମାନେ ତା’ ସହ ଟାଣି ହୋଇଯାଆନ୍ତି। ଏହି ବିପଦକୁ ଏଡ଼ାଇବାକୁ ଅନେକ ସାବଧାନତା ଅବଲମ୍ବନ କରାଯାଇପାରେ। \n",
      "pred :  ज़दिजने आरोईको ग़़ारो भपाडि दिया ज़े किन्तु ग़ोडो गुड़ारे अटिकि ज़े तेब ग़़ा दोडि पलाए ले से मने तासो तानि हो जानती एही भिपोडो को एडे वाखु अनेक सबवदना ता अबलम बन कर जई पारे\n",
      "\n",
      " \n",
      "-------3------\n",
      "true : 1995ରେ ସେ ପାର୍ଟିଜାନ୍‌ର ଇତିହାସରେ ଶ୍ରେଷ୍ଠ ଖେଳାଳି ଭାବରେ ମନୋନୀତ ହୋଇଥିଲେ । \n",
      "pred :  उने उस्वा पन्चान नवरे से पाटी जान न दो इतिया सो दे स्रष्टो खाला लिब आप नितो उदिले\n",
      "\n",
      " \n",
      "-------4------\n",
      "true : ଏବେ ବି ଏହାର ଉତ୍ପାଦନ ହେଉଛି କିନ୍ତୁ ଅଧିକ ମହତ୍ତ୍ୱପୂର୍ଣ୍ଣ ଭାବରେ ଡିଜିଟାଲ କ୍ୟାମେରାର ଇମେଜ୍ ସେନ୍ସର ଫର୍ମାଟଗୁଡ଼ିକ ଏହାର ଆସପେକ୍ଟ ରେସିଓକୁ ଉତ୍ତରାଧିକାରୀ ସୂତ୍ରରେ ପାଇଛି \n",
      "pred :  ಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿಸಿ\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "model.config.use_cache=True\n",
    "\n",
    "for idx in range(5):\n",
    "\n",
    "    target = normalize_text(asr_dataset['train'][idx]['transcription'])\n",
    "    audio_original = asr_dataset['train'][idx]['audio']['array']\n",
    "    original_sample_rate = asr_dataset['train'][idx]['audio']['sampling_rate']\n",
    "\n",
    "    audio_16000 = down_sample_audio(audio_original, original_sample_rate)\n",
    "\n",
    "    input_feature = feature_extractor(raw_speech=audio_16000,\n",
    "                                    sampling_rate=16000,\n",
    "                                    return_tensors='pt').input_features\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = model.generate(input_feature.to('cuda'), language='bengali', task='transcribe')\n",
    "\n",
    "\n",
    "    text_pred =  tokenizer.batch_decode(op,skip_special_tokens=True )[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'true : {target} \\npred : {text_pred}')\n",
    "    print('\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class whisper_training_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len):#daatset is huggingface dataset object\n",
    "        self.dataset = dataset\n",
    "        self.max_len = max_len\n",
    "        self.bos_token = model.config.decoder_start_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        audio_data = down_sample_audio(item['audio'][\"array\"], item['audio'][\"sampling_rate\"])\n",
    "        input_features = feature_extractor(audio_data, sampling_rate=16000,return_tensors='pt').input_features[0]\n",
    "\n",
    "        # Process the transcription\n",
    "        transcription = item['transcription']\n",
    "\n",
    "        # Create labels\n",
    "        labels = tokenizer(transcription, padding=\"max_length\", max_length=self.max_len, truncation=True, return_tensors=\"pt\")\n",
    "        labels = labels[\"input_ids\"].masked_fill(labels['attention_mask'].ne(1), -100)\n",
    "        labels = labels[0][1:]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = whisper_training_dataset(dataset=asr_dataset['train'], max_len=400)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust batch size as needed\n",
    "    shuffle=True,  # Shuffle data during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation(model):\n",
    "    device = 'cuda'\n",
    "\n",
    "    test_dataset = whisper_training_dataset(dataset=asr_dataset['validation'], max_len=60)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    progress_bar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        model.eval()\n",
    "        model.config.use_cache = True\n",
    "\n",
    "        input_features = batch[\"input_features\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(input_features=input_features, language='bengali', task='transcribe')\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Update global lists\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_labels)\n",
    "\n",
    "        # Compute batch WER (not cumulative)\n",
    "        batch_cer = cer(decoded_labels, decoded_preds) * 100\n",
    "        progress_bar.set_postfix(batch_CER=f\"{batch_cer:.2f}%\")\n",
    "\n",
    "    total_cer = cer(references,predictions) * 100\n",
    "    return total_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [04:10<00:00,  5.11s/it, batch_CER=924.57%] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "853.3625197616441"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WER before the training\n",
    "torch.cuda.empty_cache()\n",
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,616,832 || all params: 252,351,744 || trainable%: 4.2072\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=64, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\", \"q_proj\", \"out_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# model.save_pretrained('lora_model')\n",
    "\n",
    "# model = PeftModel.from_pretrained(model,model_id='lora_model')\n",
    "\n",
    "# for n,p in model.named_parameters():\n",
    "#     print(n,p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = whisper_training_dataset(dataset=asr_dataset['validation'], max_len=400)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,  # Adjust batch size as needed\n",
    "    shuffle=True,  # Shuffle data during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/136 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/49 [00:42<04:13,  6.04s/it, batch_CER=515.95%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m             model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 74\u001b[0m cer_value \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming evaluation returns WER\u001b[39;00m\n\u001b[1;32m     75\u001b[0m running_cer\u001b[38;5;241m.\u001b[39mappend(cer_value)\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbengali\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranscribe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m decoded_labels \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(labels, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/peft_model.py:854\u001b[0m, in \u001b[0;36mPeftModel.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    853\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[1;32m    768\u001b[0m (\n\u001b[1;32m    769\u001b[0m     seek_sequences,\n\u001b[1;32m    770\u001b[0m     seek_outputs,\n\u001b[1;32m    771\u001b[0m     should_skip,\n\u001b[1;32m    772\u001b[0m     do_condition_on_prev_tokens,\n\u001b[1;32m    773\u001b[0m     model_output_type,\n\u001b[0;32m--> 774\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:950\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[0;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    946\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    947\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[0;32m--> 950\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1776\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1772\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1773\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1774\u001b[0m         )\n\u001b[0;32m-> 1776\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1796\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1643\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1637\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1638\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1639\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1640\u001b[0m     )\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1643\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1326\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1313\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1314\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         cache_position,\n\u001b[1;32m   1324\u001b[0m     )\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1326\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:718\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    715\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:530\u001b[0m, in \u001b[0;36mWhisperSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(current_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m--> 530\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;66;03m# save all key/value_states to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[1;32m    533\u001b[0m         cache_position \u001b[38;5;241m=\u001b[39m cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/tuners/lora/layer.py:621\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 621\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dropout, nn\u001b[38;5;241m.\u001b[39mIdentity) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "device = 'cuda'\n",
    "# Filter parameters with requires_grad=True\n",
    "requires_grad_params = filter(lambda x: x[1].requires_grad, model.parameters())\n",
    "optimizer = torch.optim.AdamW(requires_grad_params, lr=5e-4)  # Only for LoRA Training\n",
    "\n",
    "max_epochs = 4\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "running_train_loss = []\n",
    "running_val_loss = []\n",
    "running_cer = []\n",
    "\n",
    "global_step = 0\n",
    "accumulated_loss = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, total=len(train_dataloader), leave=False)):\n",
    "\n",
    "        model.config.use_cache = False\n",
    "        model.train()\n",
    "        input_features, labels = batch[\"input_features\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps  # Scale the loss\n",
    "\n",
    "        loss.backward()\n",
    "        accumulated_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_train_loss.append(accumulated_loss)\n",
    "            accumulated_loss = 0\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop\n",
    "            if global_step % 50 == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in test_dataloader:\n",
    "                        val_input, val_labels = val_batch[\"input_features\"].to(device), val_batch[\"labels\"].to(device)\n",
    "                        val_outputs = model(val_input, labels=val_labels)\n",
    "                        val_loss += val_outputs.loss.item()\n",
    "                val_loss /= len(test_dataloader)\n",
    "                running_val_loss.append(val_loss)\n",
    "\n",
    "                # Plot both train and val loss\n",
    "                clear_output(wait=True)\n",
    "                plt.plot(running_train_loss, label='Training Loss')\n",
    "                plt.plot([i * 5 for i in range(len(running_val_loss))], running_val_loss, label='Validation Loss')\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.title(f'Epoch {epoch+1}, Step {step+1}')\n",
    "                plt.show()\n",
    "\n",
    "            # Save model every 100 global steps\n",
    "            if global_step % 100 == 0:\n",
    "                model.save_pretrained('lora_model')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    cer_value = evaluation(model)  # Assuming evaluation returns WER\n",
    "    running_cer.append(cer_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "import evaluate\n",
    "#pip install jiwer\n",
    "\n",
    "wer  = evaluate.load('wer')\n",
    "\n",
    "\n",
    "\n",
    "def down_sample_audio(audio_original, original_sample_rate):\n",
    "    target_sample_rate = 16000\n",
    "\n",
    "    # Calculate the number of samples for the target sample rate\n",
    "    num_samples = int(len(audio_original) * target_sample_rate / original_sample_rate)\n",
    "\n",
    "    # Resample the audio array to the target sample rate\n",
    "    downsampled_audio = resample(audio_original, num_samples)\n",
    "\n",
    "    return downsampled_audio\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",language='bengali',task='translate')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\",language='bengali',task='translate')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to('cuda')\n",
    "\n",
    "model = PeftModel.from_pretrained(model, 'lora_adapter', is_trainable=False)\n",
    "\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------0------\n",
      "true : Animals such as elephants and giraffes have a tendency to come up to take a closer look at cars and standard equipment that looks good. \n",
      "pred : Consultation is an excellent way to see the car and well seen by the hand-made animals of the pastures.\n",
      "\n",
      " \n",
      "-------1------\n",
      "true : Please treat this place with the honesty, seriousness, and respect it deserves. Don't make jokes about the Holocaust or the Nazis. \n",
      "pred : Sympathy in the place is hard to think of with deep feelings and thoughts about alcoholics or alcoholic topics.\n",
      "\n",
      " \n",
      "-------2------\n",
      "true : 108 types of Chhapan Bhog In Hinduism, 56 types of food items such as sweet fruits, almond dishes, etc. which are offered to God were offered to Baba Shyam. \n",
      "pred : The 18-day anniversary of the event was celebrated by the Hinduism and by the sweet fruit-ed-grain season as well as the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season of the season\n",
      "\n",
      " \n",
      "-------3------\n",
      "true : Currently widely available throughout the archipelago, Javanese cuisine has a long tradition of seasonal dishes that are flavored with Javanese peanut chili sugar, especially Javanese coconut sugar, and various aromatic spices. \n",
      "pred : Early in the archipelago of all Archipelago, the island has an important part in the Brazilian economy, such as the Chinese word for javanejman, china, and other related problems.\n",
      "\n",
      " \n",
      "-------4------\n",
      "true : Initially, clothing was heavily influenced by the Byzantine culture of the East. \n",
      "pred : In the beginning, the taste of the past has been influenced by the white white sandal.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "model.config.use_cache=True\n",
    "\n",
    "for idx in range(5):\n",
    "\n",
    "    target = asr_dataset['validation'][idx]['eng_translation']\n",
    "    audio_original = asr_dataset['validation'][idx]['audio']['array']\n",
    "    original_sample_rate = asr_dataset['validation'][idx]['audio']['sampling_rate']\n",
    "\n",
    "    audio_16000 = down_sample_audio(audio_original, original_sample_rate)\n",
    "\n",
    "    input_feature = feature_extractor(raw_speech=audio_16000,\n",
    "                                    sampling_rate=16000,\n",
    "                                    return_tensors='pt').input_features\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = model.generate(input_feature.to('cuda'), language='bengali', task='translate')\n",
    "\n",
    "\n",
    "    text_pred =  tokenizer.batch_decode(op,skip_special_tokens=True )[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'true : {target} \\npred : {text_pred}')\n",
    "    print('\\n ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
